slidify('testing.Rmd')
slidify('testing.Rmd')
slidify('testing.Rmd')
slidify('testing.Rmd')
slidify('testing.Rmd')
slidify('testing.Rmd')
slidify('testing.Rmd')
slidify('testing.Rmd')
slidify('testing.Rmd')
slidify('testing.Rmd')
slidify('testing.Rmd')
install.packages('xgboost')
install.packages("xgboost")
getwd()
source('utils.R')
source('utils.R')
train <- fread('../Data/train.csv', stringsAsFactors = FALSE, data.table = FALSE)
#Remove the rows which have missing Price
train <- train[!is.na(train$price),]
fwrite(train,'../Data/train.csv')
fold <- createFolds(train$price,10)
#Save fold list to RDS format. We will use it during model training.
saveRDS(fold,'../Data/fold.RDS')
source('utils.R')
#Read the data for modeling
train <- fread('../Data/train.csv',stringsAsFactors = FALSE, data.table = FALSE)
label <- train$price
train$price <- NULL
train[is.na(train)] <- -9999
trainSparse <- sparse.model.matrix(~., data=train)
dtrain <- xgb.DMatrix(trainSparse, label = price)
#Read the 10 folds created by createCrossValidation.R script
cv_folds <- readRDS('../Data/fold.RDS')
#Load useful libraries
library(data.table)
library(readr)
library(funModeling)
library(caret)
library(ranger)
library(xgboost)
library(rBayesianOptimization)
library(rCharts)
library(devtools)
library(slidify)
library(slidifyLibraries)
library(xtable)
library(knitr)
library(Matrix)
#Useful Functions
source('utils.R')
#Read the data for modeling
train <- fread('../Data/train.csv',stringsAsFactors = FALSE, data.table = FALSE)
label <- train$price
train$price <- NULL
train[is.na(train)] <- -9999
trainSparse <- sparse.model.matrix(~., data=train)
dtrain <- xgb.DMatrix(trainSparse, label = price)
#Read the 10 folds created by createCrossValidation.R script
cv_folds <- readRDS('../Data/fold.RDS')
source('utils.R')
#Read the data for modeling
train <- fread('../Data/train.csv',stringsAsFactors = FALSE, data.table = FALSE)
label <- train$price
train$price <- NULL
train[is.na(train)] <- -9999
trainSparse <- sparse.model.matrix(~., data=train)
dtrain <- xgb.DMatrix(trainSparse, label = label)
#Read the 10 folds created by createCrossValidation.R script
cv_folds <- readRDS('../Data/fold.RDS')
source('utils.R')
#Read the data for modeling
train <- fread('../Data/train.csv',stringsAsFactors = FALSE, data.table = FALSE)
label <- train$price
train$price <- NULL
train[is.na(train)] <- -9999
trainSparse <- sparse.model.matrix(~., data=train)
dtrain <- xgb.DMatrix(trainSparse, label = label)
#Read the 10 folds created by createCrossValidation.R script
cv_folds <- readRDS('../Data/fold.RDS')
#Function for performing k-fold cross-validation for XGBOOST model
xgb_cv_bayes <- function(max.depth, min_child_weight, subsample,colsample,lambda,gamma,alpha) {
cv <- xgb.cv(params = list(booster = "gbtree",
eta = 0.01,
max_depth = max.depth,
min_child_weight = min_child_weight,
subsample = subsample,
colsample_bytree = colsample,
objective = 'reg:gamma',
base_score=mean(label)),
data = dtrain,
nround = 1000,
early.stop.round = 10,
maximize = FALSE,
lambda = lambda,
gamma = gamma,
alpha = alpha,
nthread = 8,
verbose=0,
folds = cv_folds,
feval = 'gamma-deviance'
)
#rBayesianOptimization library maximizes the evaluation metric. Gamma Deviance is error metric so we take negative to minimize it.
optMetricVal <- - cv[dim(cv)[1],]$test.error.mean
list(Score = optMetricVal)
}
OPT_Res <- BayesianOptimization(xgb_cv_bayes,
bounds = list(max.depth = c(3L, 15L),
min_child_weight = c(1L, 10L),
subsample = c(0.5, 0.9),
lambda=c(0.001,2.0),
alpha=c(0.001,2.0),
gamma=c(1L,5L),
colsample=c(0.1,0.9)),
init_points = 5, n_iter = 10,
acq = "ucb", kappa = 2.576, eps = 0.0,
verbose = TRUE)
source('utils.R')
#Read the data for modeling
train <- fread('../Data/train.csv',stringsAsFactors = FALSE, data.table = FALSE)
label <- train$price
train$price <- NULL
train[is.na(train)] <- -9999
trainSparse <- sparse.model.matrix(~., data=train)
dtrain <- xgb.DMatrix(trainSparse, label = label)
#Read the 10 folds created by createCrossValidation.R script
cv_folds <- readRDS('../Data/fold.RDS')
#Function for performing k-fold cross-validation for XGBOOST model
xgb_cv_bayes <- function(max.depth, min_child_weight, subsample,colsample,lambda,gamma,alpha) {
cv <- xgb.cv(params = list(booster = "gbtree",
eta = 0.01,
max_depth = max.depth,
min_child_weight = min_child_weight,
subsample = subsample,
colsample_bytree = colsample,
objective = 'reg:gamma',
base_score=mean(label)),
data = dtrain,
nround = 1000,
early.stop.round = 10,
maximize = FALSE,
lambda = lambda,
gamma = gamma,
alpha = alpha,
nthread = 8,
verbose=0,
folds = cv_folds,
eval = 'gamma-deviance'
)
#rBayesianOptimization library maximizes the evaluation metric. Gamma Deviance is error metric so we take negative to minimize it.
optMetricVal <- - cv[dim(cv)[1],]$test.error.mean
list(Score = optMetricVal)
}
OPT_Res <- BayesianOptimization(xgb_cv_bayes,
bounds = list(max.depth = c(3L, 15L),
min_child_weight = c(1L, 10L),
subsample = c(0.5, 0.9),
lambda=c(0.001,2.0),
alpha=c(0.001,2.0),
gamma=c(1L,5L),
colsample=c(0.1,0.9)),
init_points = 5, n_iter = 10,
acq = "ucb", kappa = 2.576, eps = 0.0,
verbose = TRUE)
cv
max.depth = 3
min_child_weight = 1
subsample = 0.5
lambda = 0.001
alpha = 0.001
gamma = 1
colsample = 0.1
cv <- xgb.cv(params = list(booster = "gbtree",
eta = 0.01,
max_depth = max.depth,
min_child_weight = min_child_weight,
subsample = subsample,
colsample_bytree = colsample,
objective = 'reg:gamma',
base_score=mean(label)),
data = dtrain,
nround = 1000,
early.stop.round = 10,
maximize = FALSE,
lambda = lambda,
gamma = gamma,
alpha = alpha,
nthread = 8,
verbose=0,
folds = cv_folds,
eval = 'gamma-deviance'
)
cv
names(cv)
dim(cv)
cv$best_iteration
cv$call
cv$evaluation_log
cv$evaluation_log[cv$best_iteration]
cv$evaluation_log[cv$best_iteration]$test_gamma_nloglik_mean
cv$evaluation_log[cv$best_iteration]$test_gamma_nloglik_mean
#Function for performing k-fold cross-validation for XGBOOST model
xgb_cv_bayes <- function(max.depth, min_child_weight, subsample,colsample,lambda,gamma,alpha) {
cv <- xgb.cv(params = list(booster = "gbtree",
eta = 0.01,
max_depth = max.depth,
min_child_weight = min_child_weight,
subsample = subsample,
colsample_bytree = colsample,
objective = 'reg:gamma',
base_score=mean(label)),
data = dtrain,
nround = 1000,
early_stopping_rounds = 10,
maximize = FALSE,
lambda = lambda,
gamma = gamma,
alpha = alpha,
nthread = 8,
verbose=0,
folds = cv_folds,
eval = 'gamma-deviance'
)
#rBayesianOptimization library maximizes the evaluation metric. Gamma Deviance is error metric so we take negative to minimize it.
optMetricVal <- - cv$evaluation_log[cv$best_iteration]$test_gamma_nloglik_mean
list(Score = optMetricVal)
}
OPT_Res <- BayesianOptimization(xgb_cv_bayes,
bounds = list(max.depth = c(3L, 15L),
min_child_weight = c(1L, 10L),
subsample = c(0.5, 0.9),
lambda=c(0.001,2.0),
alpha=c(0.001,2.0),
gamma=c(1L,5L),
colsample=c(0.1,0.9)),
init_points = 5, n_iter = 10,
acq = "ucb", kappa = 2.576, eps = 0.0,
verbose = TRUE)
cv$best_iteration
cv$evaluation_log[cv$best_iteration]
names(cv)
cv$params
names(cv)
cv$callbacks
names(cv)
cv$niter
cv
class(cv)
cv$niter
names(cv)
cv$params
cv$best_ntreelimit
cv$best_iteration
cv$evaluation_log
OPT_Res
OPT_Res$Best_Par
OPT_Res$Best_Par[1]
OPT_Res$Best_Par$max_depth
OPT_Res$Best_Par['max_depth']
names(OPT_Res$Best_Par)
class(OPT_Res$Best_Par)
data.frame(OPT_Res$Best_Par)
OPT_Res$Best_Par
OPT_Res$Best_Par[1]
#Train the model with optimal parameters and save cross-validated predictions
max.depth <-  OPT_Res$Best_Par[1]
min_child_weight <- OPT_Res$Best_Par[2]
subsample <- OPT_Res$Best_Par[3]
lambda <- OPT_Res$Best_Par[4]
alpha <- OPT_Res$Best_Par[5]
gamma <- OPT_Res$Best_Par[6]
colsample <- OPT_Res$Best_Par[7]
cv <- xgb.cv(params = list(booster = "gbtree",
eta = 0.01,
max_depth = max.depth,
min_child_weight = min_child_weight,
subsample = subsample,
colsample_bytree = colsample,
objective = 'reg:gamma',
base_score=mean(label)),
data = dtrain,
nround = 1000,
early_stopping_rounds = 10,
maximize = FALSE,
lambda = lambda,
gamma = gamma,
alpha = alpha,
nthread = 8,
verbose=0,
folds = cv_folds,
eval = 'gamma-deviance',
prediction = TRUE
)
cv$pred
plot(cv$pred,price)
plot(cv$pred,label)
source('utils.R')
install.packages('Ckmeans.1d.dp')
source('utils.R')
PvO11(label,cv$pred,'Predicted vs Observed Chart')
pvoData <- data.frame(Observed=label,Predicted=cv$pred)
fwrite(pvoData,'../Data/pvoData.csv')
xgb.importance(cv)
xgb.importance(feature_names = names(trainSparse),cv)
OPT_Res
cv$best_iteration
fullModel <- xgb.cv(params = list(booster = "gbtree",
eta = 0.01,
max_depth = max.depth,
min_child_weight = min_child_weight,
subsample = subsample,
colsample_bytree = colsample,
objective = 'reg:gamma',
base_score=mean(label)),
data = dtrain,
nround = bestIteration,
maximize = FALSE,
lambda = lambda,
gamma = gamma,
alpha = alpha,
nthread = 8,
verbose=0
)
fullModel <- xgb.train(params = list(booster = "gbtree",
eta = 0.01,
max_depth = max.depth,
min_child_weight = min_child_weight,
subsample = subsample,
colsample_bytree = colsample,
objective = 'reg:gamma',
base_score=mean(label)),
data = dtrain,
nround = bestIteration,
maximize = FALSE,
lambda = lambda,
gamma = gamma,
alpha = alpha,
nthread = 8,
verbose=0
)
bestIteration <- cv$best_iteration
fullModel <- xgb.train(params = list(booster = "gbtree",
eta = 0.01,
max_depth = max.depth,
min_child_weight = min_child_weight,
subsample = subsample,
colsample_bytree = colsample,
objective = 'reg:gamma',
base_score=mean(label)),
data = dtrain,
nround = bestIteration,
maximize = FALSE,
lambda = lambda,
gamma = gamma,
alpha = alpha,
nthread = 8,
verbose=0
)
xgb.importance(feature_names = colnames(trainSparse),model=fullModel)
OPT_Res <- data.frame(OPT_Res)
OPT_Res
OPT_Res$Best_Par
names(OPT_Res$Best_Par)
values(OPT_Res$Best_Par)
OPT_Res$Best_Par$values
OPT_Res$Best_Par[1]
OPT_Res$Best_Par[[1]]
as.numeric(OPT_Res$Best_Par)
source('utils.R')
#Read Data
data <- fread('../Data/imports-85.data.txt',stringsAsFactors = FALSE)
columnNames <- c('symboling','normalized_losses','make','fuel_type',
'aspiration','num_of_doors','body_style','drive_wheels',
'engine_location','wheel_base','length','width',
'height','curb_weight','engine_type','num_of_cylinders',
'engine_size','fuel_system','bore','stroke',
'compression_ratio','horsepower','peak_rpm','city_mpg',
'highway_mpg','price')
names(data) <- columnNames
data$normalized_losses <- as.numeric(data$normalized_losses)
data$bore <- as.numeric(data$bore)
data$stroke <- as.numeric(data$stroke)
data$horsepower <- as.numeric(data$horsepower)
data$peak_rpm <- as.numeric(data$peak_rpm)
data$price <- as.numeric(data$price)
#Write to train.csv
fwrite(data,'../Data/train.csv')
#Exploratory Data Analysis
EDA <- df_status(data)
fwrite(EDA,'../Data/EDA.csv')
source('utils.R')
train <- fread('../Data/train.csv', stringsAsFactors = FALSE, data.table = FALSE)
#Remove the rows which have missing Price
train <- train[!is.na(train$price),]
fwrite(train,'../Data/train.csv')
fold <- createFolds(train$price,10)
#Save fold list to RDS format. We will use it during model training.
saveRDS(fold,'../Data/fold.RDS')
source('utils.R')
#Read the data for modeling
train <- fread('../Data/train.csv',stringsAsFactors = FALSE, data.table = FALSE)
label <- train$price
train$price <- NULL
train[is.na(train)] <- -9999
trainSparse <- sparse.model.matrix(~., data=train)
dtrain <- xgb.DMatrix(trainSparse, label = label)
#Read the 10 folds created by createCrossValidation.R script
cv_folds <- readRDS('../Data/fold.RDS')
#Function for performing k-fold cross-validation for XGBOOST model
xgb_cv_bayes <- function(max.depth, min_child_weight, subsample,colsample,lambda,gamma,alpha) {
cv <- xgb.cv(params = list(booster = "gbtree",
eta = 0.01,
max_depth = max.depth,
min_child_weight = min_child_weight,
subsample = subsample,
colsample_bytree = colsample,
objective = 'reg:gamma',
base_score=mean(label)),
data = dtrain,
nround = 1000,
early_stopping_rounds = 10,
maximize = FALSE,
lambda = lambda,
gamma = gamma,
alpha = alpha,
nthread = 8,
verbose=0,
folds = cv_folds,
eval = 'gamma-deviance'
)
#rBayesianOptimization library maximizes the evaluation metric. Gamma Deviance is error metric so we take negative to minimize it.
optMetricVal <- - cv$evaluation_log[cv$best_iteration]$test_gamma_nloglik_mean
list(Score = optMetricVal)
}
OPT_Res <- BayesianOptimization(xgb_cv_bayes,
bounds = list(max.depth = c(3L, 15L),
min_child_weight = c(1L, 10L),
subsample = c(0.5, 0.9),
lambda=c(0.001,2.0),
alpha=c(0.001,2.0),
gamma=c(1L,5L),
colsample=c(0.1,0.9)),
init_points = 5, n_iter = 10,
acq = "ucb", kappa = 2.576, eps = 0.0,
verbose = TRUE)
#Train the model with optimal parameters and save cross-validated predictions
max.depth <-  OPT_Res$Best_Par[1]
min_child_weight <- OPT_Res$Best_Par[2]
subsample <- OPT_Res$Best_Par[3]
lambda <- OPT_Res$Best_Par[4]
alpha <- OPT_Res$Best_Par[5]
gamma <- OPT_Res$Best_Par[6]
colsample <- OPT_Res$Best_Par[7]
optimalParameters <- data.frame(parameter=names(OPT_Res$Best_Par),
values=as.numeric(OPT_Res$Best_Par))
fwrite(optimalParameters,'../Data/optimalParameters.csv')
cv <- xgb.cv(params = list(booster = "gbtree",
eta = 0.01,
max_depth = max.depth,
min_child_weight = min_child_weight,
subsample = subsample,
colsample_bytree = colsample,
objective = 'reg:gamma',
base_score=mean(label)),
data = dtrain,
nround = 1000,
early_stopping_rounds = 10,
maximize = FALSE,
lambda = lambda,
gamma = gamma,
alpha = alpha,
nthread = 8,
verbose=0,
folds = cv_folds,
eval = 'gamma-deviance',
prediction = TRUE
)
pvoData <- data.frame(Observed=label,Predicted=cv$pred)
fwrite(pvoData,'../Data/pvoData.csv')
bestIteration <- cv$best_iteration
fullModel <- xgb.train(params = list(booster = "gbtree",
eta = 0.01,
max_depth = max.depth,
min_child_weight = min_child_weight,
subsample = subsample,
colsample_bytree = colsample,
objective = 'reg:gamma',
base_score=mean(label)),
data = dtrain,
nround = bestIteration,
maximize = FALSE,
lambda = lambda,
gamma = gamma,
alpha = alpha,
nthread = 8,
verbose=0
)
featureImportance <- xgb.importance(feature_names = colnames(trainSparse),model=fullModel)
fwrite(featureImportance,'../Data/featureImportance.csv')
slidify('testing.Rmd')
slidify('testing.Rmd')
slidify('testing.Rmd')
slidify('testing.Rmd')
slidify('testing.Rmd')
slidify('testing.Rmd')
slidify('testing.Rmd')
slidify('testing.Rmd')
slidify('testing.Rmd')
slidify('testing.Rmd')
slidify('testing.Rmd')
slidify('testing.Rmd')
slidify('testing.Rmd')
slidify('presentation.Rmd')
