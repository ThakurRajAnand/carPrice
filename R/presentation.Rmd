---
title       : Predicting Car Prices using Machine Learning
author      : Thakur Raj Anand
framework   : io2012 # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow      # 
widgets     : [mathjax,bootstrap,quiz,shiny,interactive]# {mathjax, quiz, bootstrap}
ext_widgets : {rCharts: [libraries/nvd3]}
mode        : selfcontained # {standalone, draft}
knit        : slidify::knit2slides
---

## OUTLINE

1. Introduction
2. Exploratory Data Analysis
3. Use Cases
4. Target Variable Distribution
5. Model Validation
6. Model Training
7. Model Performance
8. Model Deployment
9. Summary and Next Steps

---

## Intoduction
In this case study we would be exploring the data related to cars given on the following website
https://archive.ics.uci.edu/ml/datasets/Automobile


Data contains 205 rows and 26 columns. Everything is done in R including this HTML deck. All the codes related to this case study can be found in the github following Github repository
https://github.com/ThakurRajAnand/carPrice


---

## Exploratory Data Analysis
```{r echo = F, results = 'asis',message=F, warning=F}
source('utils.R')
EDA <- fread('../Data/EDA.csv',stringsAsFactors = FALSE, data.table = FALSE)
kable(EDA[1:13,],longtable=TRUE, booktabs=TRUE,row.names = TRUE)
```

---

## Exploratory Data Analysis
```{r echo = F, results = 'asis',message=F, warning=F}
kable(EDA[14:26,],longtable=TRUE, booktabs=TRUE)
```

---

## Use Cases
There can be many business use cases possible using this data or other similar datasets. Following are some examples 

1 - **Clustering Cars in diferent categories :** We can use unsupervised ML algorithms like K-means, T-SNE for this.   

2 - **Predicting Car Prices :** We can use supervised ML algortihms like Gradient Boosting Machine, Random Forest, GLMs for this.  

We can also predict other variables in this dataset based on the relevance and impact of predicting them on the business. For this study, we will go ahead with predicting Car Prices as it is a very common business problem and can have real impact for a company like **AUTO1**.

---

## Target Variable Distribution
Distribution of Price is skewed and **GAMMA** distribution might be a good choice for the model.

```{r echo = F, message=F, warning=F, fig.align='center', results = 'asis', comment = NA}

train <- fread('../Data/train.csv', stringsAsFactors = FALSE, data.table = FALSE)

plot1 <- Highcharts$new()
plot1$title(text = 'Price Distribution')
labels <- hist(train$price,plot = FALSE)$breaks
labels1 <- c()
for(i in 1:(length(labels)-1)){
  labels1[i] <- paste(labels[i],"-",labels[i+1],collapse = '')
}
plot1$xAxis(categories = labels1)
plot1$yAxis(list(
  list(title = list(text = 'Frequency'))
  ))

plot1$series(name = 'Price', type='column', data=hist(train$price,plot=FALSE)$counts, color='#6D7991', xAxis=0, yAxis=0)
plot1$series(name = 'Price', type='spline', data=hist(train$price,plot=FALSE)$counts, color='#DD6600', xAxis=0, yAxis=0, dashStyle = "shortdot")
plot1$show('iframesrc', cdn =TRUE)
```

---

## Model Validation
Two important things to decide before we start to build models are the following  

1 - **Performance Metric :** To measure the performance of our model we need a model performance metric. As we saw earlier that Price is gamma distributed and gamman deviance would be good choice for measuring the performance of our models. 

2 - **Validation Framework :** To make sure that we are not overfitting to the training data we need to use decide a validation framework like train-validation-holdout, k-fold cross-validation, time based validation. In this data we don't have any time element and hence we don't need to worry about time based validation. We will go ahead with 10-fold cross validation and will evaluate the performance of our model out of fold predictions (10 folds in this case)

---

## Model Training
There are many machine learning models which can be used. We will focus on using **XGBOOST** library to train Gradient Boosting Model. XGBOOST has lot of model parameters and details can be found on https://github.com/dmlc/xgboost/blob/master/doc/parameter.md  

To achieve good performance one need to perform parameter tuning. There are many startegies like random search, grid search and bayesian optimization. Lot of research paper have show that Bayesian optimization results in better minima and takes less time in comparison to other search. Optimal parameters are found using **rBayesianOptimization** library after 15 iterations.

---

## Model Training
**Optimal Parameters**
```{r echo = F, results = 'asis',message=F, warning=F}
optimalParameters <- fread('../Data/optimalParameters.csv',stringsAsFactors = FALSE, data.table = FALSE)
kable(optimalParameters,longtable=TRUE, booktabs=TRUE,row.names = TRUE)
```

---

## Model Performance
**Feature Importance**
```{r echo = F, results = 'asis',message=F, warning=F}
featureImportance <- fread('../Data/featureImportance.csv',stringsAsFactors = FALSE, data.table = FALSE)
kable(featureImportance,longtable=TRUE, booktabs=TRUE,row.names = TRUE)
```

---

## Model Performance
**Predicted vs Observed**
We can observe that model is not doing good when Car prices were high. This generally tend to be the case when distribution is skewed. There are many ways to improve model in such situations like bias correction, modeling after taking log of the target variable etc. Overall this seems to be a pretty good model :)
```{r echo = F, message=F, warning=F, fig.align='center', results = 'asis', comment = NA}
pvoData <- fread('../Data/pvoData.csv')
PvO11(pvoData$Observed,pvoData$Predicted,'LIFT CHART')
```


---

## Model Deployment
Next logical step is to deploy this model into production. It depends a lot on the business use case. If model won't be used in real time then batch predictions can be done easily time to time. For real time use cases API based model deployment would be a good choice. Following tutorial provides a good explantation on how to deploy model from R using API
http://www.slideshare.net/JofaiChow/deploying-your-predictive-models-as-a-service-via-domino

---

## Summary and Next Steps
In this case study, we have defined the step by step approach to solve a business problem using machine learning. There are lot of things which can de done to improve models. Following is a list of things we can do  

1 - **Pre-processing :** We can try different ways to impute missing values and see what works best  
2 - **Other Models :** We can also try other models like Random Forest, SVM, Neural Networks, GLMs  
3 - **Ensemble :** We can also train multiple models and then try to combine them which in theory should give an Ensemble which would be better atleast than any individual model  
4 - **Prescriptive :** We have already developed a predictive model but lot of times business requires us to tweak things before using the predictions e.g. During a marketing campaign a credit card company might have multiple channels like Youtube, Google, Facebook etc. then questions becomes which customer should be targeted through which channel. This can be solve by finding the underlying factors which are causing model to predict high or low and they are generally known as the **Reason Codes** in Machine Learning world.

